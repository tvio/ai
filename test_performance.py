#!/usr/bin/env python3
"""
Test vÃ½konu rÅ¯znÃ½ch Ollama modelÅ¯
"""

import requests
import time
import psutil
import os

def get_system_info():
    """ZÃ­skÃ¡ informace o systÃ©mu"""
    cpu_count = psutil.cpu_count()
    memory_gb = psutil.virtual_memory().total / (1024**3)
    
    # Kontrola GPU (jednoduchÃ¡)
    gpu_available = False
    try:
        import torch
        gpu_available = torch.cuda.is_available()
    except:
        pass
    
    return {
        'cpu_cores': cpu_count,
        'memory_gb': round(memory_gb, 1),
        'gpu_available': gpu_available
    }

def test_model_performance(model_name, prompt="NapiÅ¡ krÃ¡tkou bÃ¡sniÄku o umÄ›lÃ© inteligenci."):
    """Otestuje vÃ½kon modelu"""
    print(f"\nğŸ§ª Testuji model: {model_name}")
    print("-" * 40)
    
    # MÄ›Å™enÃ­ pamÄ›ti pÅ™ed
    memory_before = psutil.virtual_memory().used / (1024**3)
    
    # MÄ›Å™enÃ­ Äasu
    start_time = time.time()
    
    try:
        response = requests.post(
            "http://localhost:11434/api/generate",
            json={
                "model": model_name,
                "prompt": prompt,
                "stream": False
            },
            timeout=120  # 2 minuty timeout
        )
        
        end_time = time.time()
        duration = end_time - start_time
        
        # MÄ›Å™enÃ­ pamÄ›ti po
        memory_after = psutil.virtual_memory().used / (1024**3)
        memory_used = memory_after - memory_before
        
        if response.status_code == 200:
            data = response.json()
            response_text = data.get('response', '')
            response_length = len(response_text)
            
            print(f"âœ… ÃšspÄ›ch!")
            print(f"â±ï¸  ÄŒas: {duration:.2f} sekund")
            print(f"ğŸ’¾ PamÄ›Å¥: +{memory_used:.1f} GB")
            print(f"ğŸ“ DÃ©lka odpovÄ›di: {response_length} znakÅ¯")
            print(f"ğŸš€ Rychlost: {response_length/duration:.0f} znakÅ¯/sekundu")
            
            # UkÃ¡zka odpovÄ›di
            preview = response_text[:100] + "..." if len(response_text) > 100 else response_text
            print(f"ğŸ“„ UkÃ¡zka: {preview}")
            
            return {
                'success': True,
                'duration': duration,
                'memory_used': memory_used,
                'response_length': response_length
            }
        else:
            print(f"âŒ Chyba: {response.status_code}")
            return {'success': False}
            
    except requests.exceptions.Timeout:
        print("âŒ Timeout - model je pÅ™Ã­liÅ¡ pomalÃ½")
        return {'success': False, 'timeout': True}
    except Exception as e:
        print(f"âŒ Chyba: {e}")
        return {'success': False}

def main():
    print("ğŸ–¥ï¸  Test vÃ½konu Ollama modelÅ¯")
    print("=" * 50)
    
    # Informace o systÃ©mu
    system = get_system_info()
    print(f"ğŸ’» CPU: {system['cpu_cores']} jader")
    print(f"ğŸ’¾ RAM: {system['memory_gb']} GB")
    print(f"ğŸ® GPU: {'DostupnÃ©' if system['gpu_available'] else 'NedostupnÃ©'}")
    
    # DoporuÄenÃ­ podle systÃ©mu
    if system['memory_gb'] < 8:
        print("\nâš ï¸  MÃ¡te mÃ©nÄ› neÅ¾ 8 GB RAM - doporuÄuji menÅ¡Ã­ modely")
        recommended_models = ['phi', 'gemma2']
    elif system['memory_gb'] < 16:
        print("\nâš ï¸  MÃ¡te 8-16 GB RAM - stÅ™ednÃ­ modely")
        recommended_models = ['gemma2', 'llama2']
    else:
        print("\nâœ… MÃ¡te dostatek RAM - mÅ¯Å¾ete zkusit vÄ›tÅ¡Ã­ modely")
        recommended_models = ['llama2', 'mistral', 'gemma2']
    
    # Test dostupnÃ½ch modelÅ¯
    print(f"\nğŸ” Testuji doporuÄenÃ© modely: {', '.join(recommended_models)}")
    
    results = {}
    
    for model in recommended_models:
        result = test_model_performance(model)
        results[model] = result
        
        if not result.get('success'):
            print(f"ğŸ’¡ Zkuste: ollama pull {model}")
    
    # ShrnutÃ­
    print("\n" + "=" * 50)
    print("ğŸ“Š ShrnutÃ­ vÃ½sledkÅ¯:")
    
    successful_models = [m for m, r in results.items() if r.get('success')]
    
    if successful_models:
        print("âœ… FunkÄnÃ­ modely:")
        for model in successful_models:
            result = results[model]
            print(f"   {model}: {result['duration']:.1f}s, +{result['memory_used']:.1f}GB RAM")
        
        # NejlepÅ¡Ã­ model
        best_model = min(successful_models, 
                        key=lambda m: results[m]['duration'])
        print(f"\nğŸ† DoporuÄenÃ½ model: {best_model}")
    else:
        print("âŒ Å½Ã¡dnÃ½ model nefunguje")
        print("ğŸ’¡ Zkuste:")
        print("   1. ollama pull phi")
        print("   2. ollama pull gemma2")

if __name__ == "__main__":
    main() 